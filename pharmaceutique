import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# Chemin vers le répertoire contenant les fichiers
folder_path = "/content/pharmacie"

#lister les fichiers
file_path = [os.path.join(folder_path,file) for file in os.listdir(folder_path) if file.endswith('.csv')]

#lister les fichiers dans un dataframe
d = [pd.read_csv(file) for file in file_path]

print("les données de employee_survey_data")
#Afficher les données
print(d[0].head())
print()

#voir la taille des données
print("voici la taille des données:")
print(d[0].shape)
print()

#voir les infos
print("infos:")
d[0].info()
print()

#Afficher le résumé statistique
print("Voici le résuméstatistiques de emplyee_survey_data:")
d[0].describe()

print("le nombre de valeur nulles:")
d[0].isnull().sum()
print("les données de general_data")
#Afficher les données
print(d[1].head())
print()

#voir la taille des données
print("voici la taille des données:")
print(d[1].shape)
print()

print("infos:")
d[1].info()
print()

#Afficher le résumé statistique
print("Voici le résuméstatistiques de General_data:")
d[1].describe()

print("le nombre de valeur nulles:")
d[1].isnull().sum()
print("les données de in_time")
#Afficher les données
print(d[2].head())
print()

#voir la taille des données
print("voici la taille des données:")
print(d[2].shape)
print()

print("infos:")
d[2].info()
print()


#Afficher le résumé statistique
print("Voici le résuméstatistiques de in_time:")
d[2].describe()
print("le nombre de valeur nulles:")
d[2].isnull().sum()
print("les données de manager_survey_data")
#Afficher les données
print(d[3].head())
print()

#voir la taille des données
print("voici la taille des données:")
print(d[3].shape)
print()

print("infos:")
d[3].info()
print()

#Afficher le résumé statistique
print("Voici le résuméstatistiques de manager_survey_data:")
d[3].describe()

print("le nombre de valeur nulles:")
d[3].isnull().sum()
print("les données de out_time")
#Afficher les données
print(d[4].head())
print()

#voir la taille des données
print("voici la taille des données:")
print(d[4].shape)
print()

print("infos:")
d[4].info()
print()

#Afficher le résumé statistique
print("Voici le résuméstatistiques de out_time:")
d[4].describe()
print("le nombre de valeur nulles:")
d[4].isnull().sum()
import os
import pandas as pd

# Chemin des fichiers
folder_path = '/content/pharmacie'

# Liste des fichiers à traiter
files = [
    "employee_survey_data.csv",
    "general_data(5).csv",
    "in_time.csv",
    "out_time.csv",
    "manager_survey_data(3).csv"
]

# Vérifiez si les fichiers existent avant de les charger
for file in files:
    file_path = os.path.join(folder_path, file)

    # Vérification de l'existence du fichier
    if os.path.exists(file_path):
        print(f"Le fichier {file} existe, traitement en cours...")

        try:
            # Charger le fichier CSV
            data = pd.read_csv(file_path)

            # Afficher les valeurs manquantes avant remplacement
            print(f"\nValeurs manquantes dans {file} avant remplacement :")
            print(data.isnull().sum())

            # Traiter les colonnes numériques
            numeric_columns = data.select_dtypes(include=['number']).columns
            data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())

            # Traiter les colonnes non numériques (catégorielles)
            object_columns = data.select_dtypes(include=['object']).columns
            for column in object_columns:
                data[column].fillna(data[column].mode().iloc[0], inplace=True)

            # Afficher les valeurs manquantes après remplacement
            print(f"\nValeurs manquantes dans {file} après remplacement :")
            print(data.isnull().sum())

        except Exception as e:
            print(f"Erreur lors du traitement du fichier {file} : {e}")
    else:
        print(f"Erreur : Le fichier {file} n'a pas été trouvé à l'emplacement {file_path}.")

# Vérification du type de données après conversion
print(d[1].dtypes)
print(d[3].dtypes)

# Convertir les colonnes en datetime
d[1].iloc[:, 1:] = d[1].iloc[:, 1:].apply(pd.to_datetime, errors='coerce')
d[3].iloc[:, 1:] = d[3].iloc[:, 1:].apply(pd.to_datetime, errors='coerce')

print(d[1].dtypes)
print(d[3].dtypes)

# Forcer la conversion des dates en datetime pour chaque colonne (sauf EmployeeID)
for col in d[1].columns[1:]:
    d[1][col] = pd.to_datetime(d[1][col], errors='coerce')

for col in d[3].columns[1:]:
    d[3][col] = pd.to_datetime(d[3][col], errors='coerce')

# Vérification des types de données après conversion
print(d[1].dtypes)
print(d[3].dtypes)


# Fusion des données
merged_data = pd.merge(d[0], d[2], on="EmployeeID", how="inner")
merged_data = pd.merge(merged_data, d[4], on="EmployeeID", how="inner")

# Extraction des colonnes de temps en utilisant .iloc pour sélectionner les lignes et colonnes
start_times = d[3].iloc[:, 1:]  # Toutes les lignes, colonnes à partir de l'indice 1
end_times = d[1].iloc[:, 1:]    # Toutes les lignes, colonnes à partir de l'indice 1

# Assurez-vous que les colonnes sont de type datetime
start_times = start_times.apply(pd.to_datetime)
end_times = end_times.apply(pd.to_datetime)

# Calcul des heures travaillées
working_hours = (end_times - start_times).apply(lambda x: x.dt.total_seconds() / 3600, axis=1)

# Ajout de la moyenne des heures travaillées
merged_data['AverageWorkingHours'] = working_hours.mean(axis=1)

# Vérification des données fusionnées
print(merged_data.head())

# Séparation des données en variables indépendantes (X) et variable cible (y)
X = merged_data.drop('Attrition', axis=1)  # Supposons que 'Attrition' est la colonne cible
y = merged_data['Attrition']

# Gestion des variables catégorielles : encodage
X = pd.get_dummies(X, drop_first=True)  # Pour éviter la multicolinéarité (ex: transformer 'Gender' en 0 ou 1)


# Diviser les données en train et test (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Imputation des NaN avec la moyenne
imputer = SimpleImputer(strategy='mean')
X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)


# Imputer les colonnes numériques avec la médiane
imputer = SimpleImputer(strategy='median')
numeric_columns = ['NumCompaniesWorked', 'TotalWorkingYears']
X_train[numeric_columns] = imputer.fit_transform(X_train[numeric_columns])
X_test[numeric_columns] = imputer.transform(X_test[numeric_columns])

# Imputer les colonnes ordinales
ordinal_columns = ['EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance']
imputer_cat = SimpleImputer(strategy='most_frequent')
X_train[ordinal_columns] = imputer_cat.fit_transform(X_train[ordinal_columns])
X_test[ordinal_columns] = imputer_cat.transform(X_test[ordinal_columns])

# Synchroniser les indices de X_train et y_train
X_train, y_train = X_train.align(y_train, join='inner', axis=0)

# Supprimer les lignes avec des NaN dans X_train
X_train = X_train.dropna()

# Synchroniser y_train avec les indices restants de X_train
y_train = y_train.loc[X_train.index]

# Répéter pour X_test et y_test
X_test, y_test = X_test.align(y_test, join='inner', axis=0)
X_test = X_test.dropna()
y_test = y_test.loc[X_test.index]

# Vérification des NaN restants
print("NaN dans X_train :\n", X_train.isnull().sum())
print("NaN dans X_test :\n", X_test.isnull().sum())

# Standardisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
knn = KNeighborsClassifier(n_neighbors=5)  # K=5 est une valeur courante
knn.fit(X_train_scaled, y_train)
y_pred = knn.predict(X_test_scaled)

print("Précision :", accuracy_score(y_test, y_pred))
print("\nRapport de classification :\n", classification_report(y_test, y_pred))
from sklearn.linear_model import LogisticRegression

# Modèle
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Évaluation
y_pred = model.predict(X_test_scaled)
from sklearn.metrics import accuracy_score
print("Accuracy:", accuracy_score(y_test, y_pred))

y_pred = model.predict(X_test_scaled)
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score
print("Accuracy:", accuracy_score(y_test, y_pred))

from sklearn.metrics import roc_auc_score, roc_curve
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
print("ROC-AUC Score:", roc_auc_score(y_test, y_pred_proba))

# Conversion de 'Yes' en 1 et 'No' en 0 dans y_test
y_test_numeric = y_test.map({'No': 0, 'Yes': 1})

# Calcul de la courbe ROC
from sklearn.metrics import roc_curve

fpr, tpr, _ = roc_curve(y_test_numeric, y_pred_proba)
plt.plot(fpr, tpr, label="ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

y_test_numeric
# Conversion manuelle
y_train = y_train.replace({'No': 0, 'Yes': 1})
y_test = y_test.replace({'No': 0, 'Yes': 1})
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_train_scaled, y_train)

y_predict = lin_reg.predict(X_test_scaled)
# Réalisation de la prédiction avec un modèle de régression linéaire (lin_reg)
print("Predictions:", y_predict)  # Prédictions basées sur les données préparées
from sklearn.metrics import mean_squared_error, r2_score
# Évaluer la performance
mse = mean_squared_error(y_test, y_predict)  # Erreur quadratique moyenne
r2 = r2_score(y_test, y_predict)            # Coefficient de détermination

print("Mean Squared Error (MSE):", mse)
print("Coefficient of Determination (R²):", r2)
# Tracer le graphe
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_predict, alpha=0.6, color='blue', label="Prédictions")
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label="Idéal (y_test = y_predict)")
plt.title("Comparaison des valeurs réelles et des prédictions")
plt.xlabel("Valeurs réelles (y_test)")
plt.ylabel("Valeurs prédites (y_predict)")
plt.legend()
plt.grid(True)
plt.show()